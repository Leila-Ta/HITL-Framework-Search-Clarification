import pandas as pd
import cohere
import re
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr
import time

# Load the MIMICS-Duo dataset
data = pd.read_csv('.../New Research/DataSet/Task3-AspectLabelling.csv')

# Take just first 5 samples for testing (you can adjust this for a full run)
# data = data.head(5)

queries = data['query']
clarifications = data['question']
labels = data['Importance Order']
# options = [data['option_1']]
options = [data['option_1'], data['option_2'], data['option_3'], data['option_4'], data['option_5']]
print("*Done(Data Loading)")

# Set up Cohere client
API_KEY = '...'
co = cohere.Client(API_KEY)

predictions = []
confidences = []

# Prepare input for Cohere Command R
for idx, (query, clarification, *options_row) in enumerate(zip(queries, clarifications, *options)):
    # Construct input prompt for Cohere Command R
    options_str = [str(option) if pd.notna(option) else '' for option in options_row]

    input_prompt = f"""

        A query and associated multi-choice clarification question with its options are shown to you. The Options of a clarification question have the correct order if the most relevant and important Options based on the different aspects of the query are positioned from left to right. 
Number 1 means you strongly disagree that the multi-choice clarification question has the correct order for the given query.
Number 2 means you somewhat disagree that the multi-choice clarification question has the correct order for the given query.
Number 3 means you neither agree nor disagree that the multi-choice clarification question has the correct order for the given query.
Number 4 means you somewhat agree that the multi-choice clarification question has the correct order for the given query.
Number 5 means you strongly agree that the multi-choice clarification question has the correct order for the given query.

    Query: {query}
    Clarification Question: {clarification}
    Options: {', '.join(options_str)}

        Now, Based on the above Query, you are tasked to rate the extent to which the most relevant and important options of the clarification question are positioned from left to right.
        Your response should only contain a single number between 1 and 5 and nothing else.
        Additionally, rate your confidence in your response on a scale of 0% to 100%, where 0% means no confidence and 100% means absolute confidence.
        provide your prediction for the label (a single number between 1 and 5) and your confidence level (as a percentage).
        Respond strictly in this format:
        Prediction: [1-5]
        Confidence: [0-100]%
         """

    try:
        # Make the API request to get a response from Cohere
        response = co.generate(
            model='command-xlarge',
            prompt=input_prompt,
            max_tokens=100,
            temperature=0,
        )

        # Extract response text
        raw_response = response.generations[0].text.strip()
        print(f"Raw Response Content for Prompt {idx + 1}: {raw_response}")

        # Extract response content
        match_prediction = re.search(r'Prediction:\s*([1-5])', raw_response)
        match_confidence = re.search(r'Confidence:\s*(\d+)%', raw_response)

        predicted_label = int(match_prediction.group(1)) if match_prediction else None
        confidence = int(match_confidence.group(1)) if match_confidence else None

        predictions.append(predicted_label)
        confidences.append(confidence)

        print(f"Predicted Label for Prompt {idx + 1}: {predicted_label}")
        print(f"Confidence Level for Prompt {idx + 1}: {confidence}%")

    except Exception as e:
        # Handle failed API call
        print(f"Error for prompt {idx + 1}: {str(e)}")
        predictions.append(None)
        confidences.append(None)

print("**Done(Prediction)")

# Convert predictions to integers for numerical comparison
predictions = [pred for pred in predictions if pred is not None]
confidences = [conf for conf in confidences if conf is not None]
print("Predictions:")
print(predictions)
print("confidences:")
print(confidences)
print("****Done(Post-Processing Predictions)")

# Convert original labels to integers
true_labels = labels[:len(predictions)].astype(int).tolist()

# Calculate metrics if there are enough predictions
if len(true_labels) > 0:
    accuracy = accuracy_score(true_labels, predictions)
    kappa = cohen_kappa_score(true_labels, predictions)
    correlation = pearsonr(true_labels, predictions)[0] if len(true_labels) > 1 else None

    avg_confidence = sum([c for c in confidences if c is not None]) / len(confidences) if confidences else 0

    print(f'Accuracy: {accuracy:.4f}')
    print(f'Cohenâ€™s Kappa: {kappa:.4f}')
    if correlation is not None:
        print(f'Pearson Correlation: {correlation:.4f}')
    else:
        print("Not enough data points to calculate Pearson correlation.")
    print(f'Average Confidence: {avg_confidence:.2f}%')

    # Calculate confidence-weighted accuracy
    weighted_correctness = 0
    total_weights = 0
    for i, (true_label, pred, conf) in enumerate(zip(true_labels, predictions, confidences)):
        if conf is not None:
            weight = conf / 100
            correctness = 1 if pred == true_label else 0
            weighted_correctness += correctness * weight
            total_weights += weight

    confidence_weighted_accuracy = weighted_correctness / total_weights if total_weights > 0 else 0
    print(f"Confidence-Weighted Accuracy: {confidence_weighted_accuracy:.4f}")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")
