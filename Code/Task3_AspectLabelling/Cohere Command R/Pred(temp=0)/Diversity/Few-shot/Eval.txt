import pandas as pd
import cohere
import re
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr
import time

# Load the MIMICS-Duo dataset
data = pd.read_csv('/.../DataSet/Task3-AspectLabelling.csv')

# Take just first 5 samples for testing (you can adjust this for a full run)
# data = data.head(5)

queries = data['query']
clarifications = data['question']
labels = data['Diversity']
# options = [data['option_1']]
options = [data['option_1'], data['option_2'], data['option_3'], data['option_4'], data['option_5']]
print("*Done(Data Loading)")

# Set up Cohere client
API_KEY = '...'
co = cohere.Client(API_KEY)

predictions = []
confidences = []

# Prepare input for Cohere Command R
for idx, (query, clarification, *options_row) in enumerate(zip(queries, clarifications, *options)):
    # Construct input prompt for Cohere Command R
    options_str = [str(option) if pd.notna(option) else '' for option in options_row]

    input_prompt = f"""

        A query and associated multi-choice clarification question with its options are shown to you. A multi-choice clarification question has high diversity if its options do not contain redundant information for the given query. You are tasked to rate the extent to which the clarification question with its options does not contain redundant information on a scale from 1 to 5 based on the query. 
    Number 1 means you strongly disagree that the multi-choice clarification question with its options has a high diversity for the given query.
    Number 2 means you somewhat disagree that the multi-choice clarification question with its options has a high diversity for the given query.
    Number 3 means you neither agree nor disagree that the multi-choice clarification question with its options has a high diversity for the given query.
    Number 4 means you somewhat agree that the multi-choice clarification question with its options has a high diversity for the given query.
    Number 5 means you strongly agree that the multi-choice clarification question with its options has a high diversity for the given query.
    Here are five examples showing how humans (users) labelled the diversity of clarification questions with its options associated with the queries.

    Example 1:
    Query: 123 solitaire
    Clarification question: Select one to refine your search
    Option 1: 123 solitaire for windows 10
    Option 2: 123 solitaire for windows 8
    Diversity label (Ground Truth Label): 1

    Example 2:
    Query: 13 inch laptop
    Clarification question: Select one to refine your search
    Option 1: 13 inch laptop windows 10
    Option 2: 13 inch laptop windows 7
    Diversity label (Ground Truth Label): 2

    Example 3:
    Query: a man called ove
    Clarification question: Select one to refine your search
    Option 1: a man called ove review
    Option 2: characters
    Option 3: quotes
    Option 4: book
    Option 5: audiobook
    Diversity label (Ground Truth Label): 3

    Example 4:
    Query: 0x80070422
    Clarification question: Select one to refine your search
    Option 1: 0x80070422 windows 10
    Option 2: 0x80070422 windows 7
    Option 3: 0x80070422 windows 8
    Option 4: 0x80070422 windows server 2016
    Diversity label (Ground Truth Label): 4

    Example 5:
    Query: 1964 corvette
    Clarification question: Select one to refine your search
    Option 1: specs 
    Option 2: 1964 corvette for sale
    Option 3: radio
    Option 4: 1964 corvette battery
    Option 5: fuel pump
    Diversity label (Ground Truth Label): 5


        Query: {query}
        Clarification Question: {clarification}
        Options: {', '.join(options_str)}

        Now, Based on the above Query, You are tasked to rate the extent to which the Clarification Question with its Options does not contain redundant information on a scale from 1 to 5. Your response should only contain a single number between 1 and 5 and nothing else.
        Additionally, rate your confidence in your response on a scale of 0% to 100%, where 0% means no confidence and 100% means absolute confidence.
        provide your prediction for the label (a single number between 1 and 5) and your confidence level (as a percentage).
        Respond strictly in this format:
        Prediction: [1-5]
        Confidence: [0-100]%
         """

    try:
        # Make the API request to get a response from Cohere
        response = co.generate(
            model='command-xlarge',
            prompt=input_prompt,
            max_tokens=100,
            temperature=0,
        )

        # Extract response text
        raw_response = response.generations[0].text.strip()
        print(f"Raw Response Content for Prompt {idx + 1}: {raw_response}")

        # Extract response content
        match_prediction = re.search(r'Prediction:\s*([1-5])', raw_response)
        match_confidence = re.search(r'Confidence:\s*(\d+)%', raw_response)

        predicted_label = int(match_prediction.group(1)) if match_prediction else None
        confidence = int(match_confidence.group(1)) if match_confidence else None

        predictions.append(predicted_label)
        confidences.append(confidence)

        print(f"Predicted Label for Prompt {idx + 1}: {predicted_label}")
        print(f"Confidence Level for Prompt {idx + 1}: {confidence}%")

    except Exception as e:
        # Handle failed API call
        print(f"Error for prompt {idx + 1}: {str(e)}")
        predictions.append(None)
        confidences.append(None)

print("**Done(Prediction)")

# Convert predictions to integers for numerical comparison
predictions = [pred for pred in predictions if pred is not None]
confidences = [conf for conf in confidences if conf is not None]
print("Predictions:")
print(predictions)
print("confidences:")
print(confidences)
print("****Done(Post-Processing Predictions)")

# Convert original labels to integers
true_labels = labels[:len(predictions)].astype(int).tolist()

# Calculate metrics if there are enough predictions
if len(true_labels) > 0:
    accuracy = accuracy_score(true_labels, predictions)
    kappa = cohen_kappa_score(true_labels, predictions)
    correlation = pearsonr(true_labels, predictions)[0] if len(true_labels) > 1 else None

    avg_confidence = sum([c for c in confidences if c is not None]) / len(confidences) if confidences else 0

    print(f'Accuracy: {accuracy:.4f}')
    print(f'Cohenâ€™s Kappa: {kappa:.4f}')
    if correlation is not None:
        print(f'Pearson Correlation: {correlation:.4f}')
    else:
        print("Not enough data points to calculate Pearson correlation.")
    print(f'Average Confidence: {avg_confidence:.2f}%')

    # Calculate confidence-weighted accuracy
    weighted_correctness = 0
    total_weights = 0
    for i, (true_label, pred, conf) in enumerate(zip(true_labels, predictions, confidences)):
        if conf is not None:
            weight = conf / 100
            correctness = 1 if pred == true_label else 0
            weighted_correctness += correctness * weight
            total_weights += weight

    confidence_weighted_accuracy = weighted_correctness / total_weights if total_weights > 0 else 0
    print(f"Confidence-Weighted Accuracy: {confidence_weighted_accuracy:.4f}")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")

