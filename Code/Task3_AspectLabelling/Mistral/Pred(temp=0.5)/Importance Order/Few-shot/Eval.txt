import time
import pandas as pd
import re
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from huggingface_hub import login
import openai
from google.colab import drive
import torch

torch.cuda.empty_cache()
torch.cuda.memory_summary(device=None, abbreviated=False)  # Optional: Displays memory status

drive.mount('/content/drive')

# Use your Hugging Face token here
login("...")

# Load model
model_name = "mistralai/Mistral-7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    device_map="auto"
).half()

# Create generator pipeline
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200,
    temperature=0.5,
    do_sample=True,
    truncation=True
)

# Load dataset
data = pd.read_csv('/content/drive/My Drive/NewResearch/DataSet/Task3-AspectLabelling.csv')

queries = data['query']
clarifications = data['question']
labels = data['Importance Order']
options = [data[f'option_{i+1}'] for i in range(5)]

print("*Done(Data Loading)*")

predictions = []
confidences = []
true_labels = []
querylist = []

# Iterate over queries
for idx, (query, clarification, *options_row) in enumerate(zip(queries, clarifications, *options)):
    try:
        # Prepare options string
        options_str = [str(option) if pd.notna(option) else '' for option in options_row]
        input_prompt = f"""

        A query and associated multi-choice clarification question with its options are shown to you. The Options of a clarification question have the correct order if the most relevant and important Options based on the different aspects of the query are positioned from left to right.
Number 1 means you strongly disagree that the multi-choice clarification question has the correct order for the given query.
Number 2 means you somewhat disagree that the multi-choice clarification question has the correct order for the given query.
Number 3 means you neither agree nor disagree that the multi-choice clarification question has the correct order for the given query.
Number 4 means you somewhat agree that the multi-choice clarification question has the correct order for the given query.
Number 5 means you strongly agree that the multi-choice clarification question has the correct order for the given query.
Here are five examples showing how humans (users) labelled the correct order of the options of the clarification questions associated with the queries.

Example 1:
Query: 2000 toyota camry
Clarification question: Select one to refine your search
Option 1: accessories
Option 2: 2000 toyota camry review
Option 3: specs
Option 4: manual
Correct order label (Ground Truth Label): 1

Example 2:
Query: 2007 honda accord
Clarification question: Select one to refine your search
Option 1: battery
Option 2: 2007 honda accord for sale
Option 3: 2007 honda accord facts
Option 4: 2007 honda accord lifespan
Correct order label (Ground Truth Label): 2

Example 3:
Query: 13 inch laptop
Clarification question: Select one to refine your search
Option 1: asus 13 inch laptop
Option 2: lenovo 13 inch laptop
Option 3: hp 13 inch laptop
Option 4: dell 13 inch laptop
Option 5: acer 13 inch laptop
Correct order label (Ground Truth Label): 3

Example 4:
Query: 144hz monitor
Clarification question: Select one to refine your search
Option 1: hp 144hz monitor
Option 2: dell 144hz monitor
Option 3: asus 144hz monitor
Option 4: asus 144hz monitor
Option 5: acer 144hz monitor
Correct order label (Ground Truth Label): 4

Example 5:
Query: create a drop down list in excel
Clarification question: What version of Excel are you looking for?
Option 1: create a drop down list in excel 2016
Option 2: create a drop down list in excel 2010
Option 3: excel 2013
Option 4: excel 2007
Correct order label (Ground Truth Label): 5


    Query: {query}
    Clarification Question: {clarification}
    Options: {', '.join(options_str)}

        Now, Based on the above Query, you are tasked to rate the extent to which the most relevant and important options of the clarification question are positioned from left to right.
        Your response should only contain a single number between 1 and 5 and nothing else.
        Additionally, rate your confidence in your response on a scale of 0% to 100%, where 0% means no confidence and 100% means absolute confidence.
        provide your prediction for the label (a single number between 1 and 5) and your confidence level (as a percentage).
        Respond strictly in this format:
        Prediction: [1-5]
        Confidence: [0-100]%
         """

        # Generate predictions
        response = generator(input_prompt, num_return_sequences=1)
        raw_response = response[0]['generated_text']
        print(f"Raw Response for Query '{query}':\n{raw_response}")

        # Extract predictions and confidences
        clarification_matches = re.findall(r'Prediction:\s*([1-5])\s*Confidence:\s*(\d+)%', raw_response)
        for prediction, confidence in clarification_matches:
            predictions.append(int(prediction))
            confidences.append(int(confidence))
            querylist.append(query)
            true_labels.append(labels.iloc[idx])  # Changed from labels[idx] to labels.iloc[idx]

    except Exception as e:
        print(f"Error for query '{query}' at index {idx}: {str(e)}")
        predictions.append(None)
        confidences.append(None)
        true_labels.append(None)

print("Predictions:")
print(predictions)
print("confidences:")
print(confidences)
print(querylist)

print("**Done(Prediction)**")

# Filter out invalid predictions
valid_indices = [i for i, p in enumerate(predictions) if p is not None]
filtered_predictions = [predictions[i] for i in valid_indices]
filtered_confidences = [confidences[i] for i in valid_indices]
filtered_true_labels = [true_labels[i] for i in valid_indices]

# Calculate metrics
if filtered_true_labels:
    accuracy = accuracy_score(filtered_true_labels, filtered_predictions)
    print(f"Accuracy: {accuracy:.4f}")
    kappa = cohen_kappa_score(filtered_true_labels, filtered_predictions)
    print(f"Cohenâ€™s Kappa: {kappa:.4f}")
    if len(filtered_true_labels) > 1:
        correlation = pearsonr(filtered_true_labels, filtered_predictions)[0]
        print(f"Pearson Correlation: {correlation:.4f}")
    avg_confidence = sum(filtered_confidences) / len(filtered_confidences)
    print(f"Average Confidence: {avg_confidence:.2f}%")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")

