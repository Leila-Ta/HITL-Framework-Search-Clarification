import time
import pandas as pd
import re
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from huggingface_hub import login
import openai
from google.colab import drive
import torch

torch.cuda.empty_cache()
torch.cuda.memory_summary(device=None, abbreviated=False)  # Optional: Displays memory status


drive.mount('/content/drive')

# Use your Hugging Face token here
login("...")

# Load model with memory optimization
model_name = "mistralai/Mistral-7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    device_map="auto"  # Offload model across CPU and GPU
).half()

# Disable gradients (inference only)
torch.set_grad_enabled(False)

# Create generator pipeline
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=2000,
    temperature=0.5,
    do_sample=True,
    truncation=True
)


# Clear cache
torch.cuda.empty_cache()

# Load the MIMICS-Duo dataset
data = pd.read_csv('/content/drive/My Drive/New Research/DataSet/Task3-AspectLabelling.csv')

queries = data['query']
clarifications = data['question']
labels = data['Coverage']  # This is for comparison after GPT-4 predicts

options = [data['option_1'], data['option_2'], data['option_3'], data['option_4'], data['option_5']]
print("*Done(Data Loading)")

# Set your OpenAI API key
openai.api_key = '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'

predictions = []
confidences = []
true_labels = []
querylist = []

# Prepare a sample input for GPT-4
for idx, (query, clarification, *options_row) in enumerate(zip(queries, clarifications, *options)):
    # Construct input prompt for Cohere Command R
    options_str = [str(option) if pd.notna(option) else '' for option in options_row]
    input_prompt = f"""

        A query and associated multi-choice clarification question with its options are shown to you. A multi-choice clarification question has high coverage if its potential answers cover every potential aspect of the query. You are tasked to rate the extent to which the clarification question and its options cover every potential aspect of the query on a scale from 1 to 5.
    Number 1 means you strongly disagree that the multi-choice clarification question with its options has a high coverage for the given query.
    Number 2 means you somewhat disagree that the multi-choice clarification question with its options has a high coverage for the given query.
    Number 3 means you neither agree nor disagree that the multi-choice clarification question with its options has a high coverage for the given query.
    Number 4 means you somewhat agree that the multi-choice clarification question with its options has a high coverage for the given query.
    Number 5 means you strongly agree that the multi-choice clarification question with its options has a high coverage for the given query.

    Here are five examples showing how humans (users) labelled the coverage of clarification questions associated with the queries.

    Example 1:
    Query: 2019 vw
    Clarification Question: Select one to refine your search
    Option 1: beetle
    Option 2: jetta
    Coverage label (Ground Truth Label): 1

    Example 2:
    Query: 1800 calorie diet
    Clarification Question: Select one to refine your search
    Option 1: printable for men
    Option 2: printable for women
    Coverage label (Ground Truth Label): 2


    Example 3:
    Query: 2k servers
    Clarification Question: Select one to refine your search
    Option 1: nba 2k servers
    Option 2: wwe 2k servers
    Option 3: nfl 2k servers
    Coverage label (Ground Truth Label): 3

    Example 4:
    Query: 2007 honda accord
    Clarification Question: Select one to refine your search
    Option 1: 2007 honda accord interior
    Option 2: 2007 honda accord exterior
    Coverage label (Ground Truth Label): 4

    Example 5:
    Query: a million little things
    Clarification Question: Select one to refine your search
    Option 1: a million little things cast
    Option 2: a million little things trailer
    Option 3: review
    Option 4: episodes
    Option 5: soundtrack
    Coverage label (Ground Truth Label): 5

        Query: {query}
        Clarification Question: {clarification}
        Options: {', '.join(options_str)}

        Now, Based on the above Query, I want to ask you to rate the extent to which the Clarification Question and its Options cover every potential aspect of the query on a scale from 1 to 5. Your response should only contain a single number between 1 and 5 and nothing else.
        Additionally, rate your confidence in your response on a scale of 0% to 100%, where 0% means no confidence and 100% means absolute confidence.
        provide your prediction for the label (a single number between 1 and 5) and your confidence level (as a percentage).
        Respond strictly in this format:
        Prediction: [1-5]
        Confidence: [0-100]%
         """


    try:
        # Generate predictions using Mistral
        response = generator(
            input_prompt,
            max_length=2000,         # Maximum length of the generated text
            num_return_sequences=1,  # Number of output sequences
        )

        # Extract response text
        raw_response = response[0]['generated_text']
        print(f"Raw Response Content for Query '{query}':\n{raw_response}")

        # Extract response content
        match_prediction = re.search(r'Prediction:\s*([1-5])', raw_response)
        match_confidence = re.search(r'Confidence:\s*(\d+)%', raw_response)

        predicted_label = int(match_prediction.group(1)) if match_prediction else None
        confidence = int(match_confidence.group(1)) if match_confidence else None

        predictions.append(predicted_label)
        confidences.append(confidence)
        querylist.append(query)
        true_labels.append(labels[idx])  # Append the corresponding true label

    except Exception as e:
        print(f"Error for query '{query}': {str(e)}")
        predictions.extend([None])
        confidences.extend([None])
        true_labels.append(None)  # Append None to true_labels for consistency


print("**Done(Prediction)**")

# Filter out invalid predictions and confidences
valid_indices = [i for i, p in enumerate(predictions) if p is not None]
filtered_predictions = [predictions[i] for i in valid_indices]
filtered_confidences = [confidences[i] for i in valid_indices]
filtered_true_labels = [true_labels[i] for i in valid_indices]

print("Predictions:")
print(predictions)
print("Confidences:")
print(confidences)
print(querylist)

# Calculate metrics
if len(filtered_true_labels) > 0:
    # Accuracy
    accuracy = accuracy_score(filtered_true_labels, filtered_predictions)
    print(f"Accuracy: {accuracy:.4f}")

    # Cohen's Kappa
    kappa = cohen_kappa_score(filtered_true_labels, filtered_predictions)
    print(f"Cohenâ€™s Kappa: {kappa:.4f}")

    # Pearson Correlation
    if len(filtered_true_labels) > 1:
        correlation = pearsonr(filtered_true_labels, filtered_predictions)[0]
        print(f"Pearson Correlation: {correlation:.4f}")
    else:
        print("Not enough data points to calculate Pearson correlation.")

    # Average Confidence
    avg_confidence = sum(filtered_confidences) / len(filtered_confidences)
    print(f"Average Confidence: {avg_confidence:.2f}%")

    # Confidence-Weighted Accuracy
    weighted_correctness = sum(
        (filtered_predictions[i] == filtered_true_labels[i]) * (filtered_confidences[i] / 100)
        for i in range(len(filtered_predictions))
    )
    total_weights = sum(filtered_confidences[i] / 100 for i in range(len(filtered_confidences)))
    confidence_weighted_accuracy = weighted_correctness / total_weights if total_weights > 0 else 0
    print(f"Confidence-Weighted Accuracy: {confidence_weighted_accuracy:.4f}")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")
