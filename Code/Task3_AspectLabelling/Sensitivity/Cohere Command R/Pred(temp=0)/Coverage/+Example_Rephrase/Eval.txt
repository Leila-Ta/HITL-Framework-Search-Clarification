import pandas as pd
import cohere
import re
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr
import time

# Load the MIMICS-Duo dataset
data = pd.read_csv('/New Research/DataSet/Task3-AspectLabelling.csv')

# Take just first 5 samples for testing (you can adjust this for a full run)
# data = data.head(5)

queries = data['query']
clarifications = data['question']
labels = data['Coverage']
# options = [data['option_1']]
options = [data['option_1'], data['option_2'], data['option_3'], data['option_4'], data['option_5']]
print("*Done(Data Loading)")

# Set up Cohere client
API_KEY = 'EWQUf6rOHYMAF00jZm2j0abE28sjIJpPrGNDDe47'
co = cohere.Client(API_KEY)

predictions = []
confidences = []
querylist = []

# Prepare input for Cohere Command R
for idx, (query, clarification, *options_row) in enumerate(zip(queries, clarifications, *options)):
    # Construct input prompt for Cohere Command R
    options_str = [str(option) if pd.notna(option) else '' for option in options_row]

    input_prompt = f"""
A query and its associated multi-choice clarification question with options are presented to you. The goal is to determine how comprehensively the clarification question and its options address every possible aspect of the query. Rate this coverage on a scale from 1 to 5.

1: The clarification question and options do not adequately cover the aspects of the query.
2: The coverage of the query by the clarification question and options is somewhat limited.
3: The clarification question and options moderately cover the aspects of the query.
4: The clarification question and options cover most aspects of the query.
5: The clarification question and options fully cover all potential aspects of the query.
Below are examples illustrating human-labeled coverage ratings for various queries and their associated clarification questions and options:

Examples
Query: 2019 vw
Clarification Question: Select one to refine your search
Options: beetle, jetta
Coverage label (Ground Truth): 1

Query: 1800 calorie diet
Clarification Question: Select one to refine your search
Options: printable for men, printable for women
Coverage label (Ground Truth): 2

Query: 2k servers
Clarification Question: Select one to refine your search
Options: nba 2k servers, wwe 2k servers, nfl 2k servers
Coverage label (Ground Truth): 3

Query: 2007 honda accord
Clarification Question: Select one to refine your search
Options: 2007 honda accord interior, 2007 honda accord exterior
Coverage label (Ground Truth): 4

Query: a million little things
Clarification Question: Select one to refine your search
Options: cast, trailer, review, episodes, soundtrack
Coverage label (Ground Truth): 5

Task:
Given the following query and clarification question with its options, rate the extent to which the clarification question and its options cover all aspects of the query.

Query: {query}
Clarification Question: {clarification}
Options: {', '.join(options_str)}

Instructions:
Provide a single numeric rating between 1 and 5, representing the coverage level.
Rate your confidence in this rating on a scale of 0% to 100%.
Format your response as:
Prediction: [1-5]
Confidence: [0-100]%
"""

    try:
        # Make the API request to get a response from Cohere
        response = co.generate(
            model='command-xlarge',
            prompt=input_prompt,
            max_tokens=1000,
            temperature=0,
        )

        # Extract response text
        raw_response = response.generations[0].text.strip()
        print(f"Raw Response Content for Prompt {idx + 1}: {raw_response}")

        # Extract response content
        match_prediction = re.search(r'Prediction:\s*([1-5])', raw_response)
        match_confidence = re.search(r'Confidence:\s*(\d+)%', raw_response)

        predicted_label = int(match_prediction.group(1)) if match_prediction else None
        confidence = int(match_confidence.group(1)) if match_confidence else None

        predictions.append(predicted_label)
        confidences.append(confidence)
        querylist.append(query)

        print(f"Predicted Label for Prompt {idx + 1}: {predicted_label}")
        print(f"Confidence Level for Prompt {idx + 1}: {confidence}%")

    except Exception as e:
        # Handle failed API call
        print(f"Error for prompt {idx + 1}: {str(e)}")
        predictions.append(None)
        confidences.append(None)

print("**Done(Prediction)")

# Convert predictions to integers for numerical comparison
predictions = [pred for pred in predictions if pred is not None]
confidences = [conf for conf in confidences if conf is not None]
print("Predictions:")
print(predictions)
print("confidences:")
print(confidences)
print("****Done(Post-Processing Predictions)")
print(querylist)

# Convert original labels to integers
true_labels = labels[:len(predictions)].astype(int).tolist()

# Calculate metrics if there are enough predictions
if len(true_labels) > 0:
    accuracy = accuracy_score(true_labels, predictions)
    kappa = cohen_kappa_score(true_labels, predictions)
    correlation = pearsonr(true_labels, predictions)[0] if len(true_labels) > 1 else None

    avg_confidence = sum([c for c in confidences if c is not None]) / len(confidences) if confidences else 0

    print(f'Accuracy: {accuracy:.4f}')
    print(f'Cohen’s Kappa: {kappa:.4f}')
    if correlation is not None:
        print(f'Pearson Correlation: {correlation:.4f}')
    else:
        print("Not enough data points to calculate Pearson correlation.")
    print(f'Average Confidence: {avg_confidence:.2f}%')

    # Calculate confidence-weighted accuracy
    weighted_correctness = 0
    total_weights = 0
    for i, (true_label, pred, conf) in enumerate(zip(true_labels, predictions, confidences)):
        if conf is not None:
            weight = conf / 100
            correctness = 1 if pred == true_label else 0
            weighted_correctness += correctness * weight
            total_weights += weight

    confidence_weighted_accuracy = weighted_correctness / total_weights if total_weights > 0 else 0
    print(f"Confidence-Weighted Accuracy: {confidence_weighted_accuracy:.4f}")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")

--------------------------------------------------
Accuracy: 0.3491
Cohen’s Kappa: 0.0134
Pearson Correlation: 0.2095
Average Confidence: 93.13%
Confidence-Weighted Accuracy: 0.3625
