import time
import pandas as pd
import re
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from huggingface_hub import login
import openai
from google.colab import drive
import torch

torch.cuda.empty_cache()
torch.cuda.memory_summary(device=None, abbreviated=False)  # Optional: Displays memory status


drive.mount('/content/drive')

# Use your Hugging Face token here
login("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")

# Load model with memory optimization
model_name = "mistralai/Mistral-7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    device_map="auto"  # Offload model across CPU and GPU
).half()

# Disable gradients (inference only)
torch.set_grad_enabled(False)

# Create generator pipeline
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200,
    temperature=1,
    do_sample=True,
    truncation=True
)


# Clear cache
torch.cuda.empty_cache()

# Load the MIMICS-Duo dataset
data = pd.read_csv('/content/drive/My Drive/New Research/DataSet/Task1-OfflineRating.csv')

queries = data['query']
clarifications = data['question']
labels = data['offline rating']  # This is for comparison after GPT-4 predicts

options = [data['option_1'], data['option_2'], data['option_3'], data['option_4'], data['option_5']]
print("*Done(Data Loading)")

# Set your OpenAI API key
openai.api_key = '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'

predictions = []
confidences = []
true_labels = []
querylist = []

# Group by queries to process all clarifications for a given query together
grouped_data = data.groupby("query")

for query, group in grouped_data:
    clarifications = group["question"].tolist()
    options_list = [
        [group.iloc[i][f"option_{j + 1}"] for j in range(5) if pd.notna(group.iloc[i][f"option_{j + 1}"])]
        for i in range(len(group))
    ]
    ground_truth = group["offline rating"].tolist()
    true_labels.extend(ground_truth)

    # Build the prompt
    clarifications_str = ""
    for idx, (clarification, options) in enumerate(zip(clarifications, options_list)):
        options_str = ", ".join(options)
        clarifications_str += (
            f"Clarification {idx + 1}: {clarification}\n"
            f"Options: {options_str}\n"
        )

    input_prompt = f"""

    You are tasked with rating how likely it is that a user (human) would prefer to engage with a clarification question and its associated options for a given query, compared to other clarification questions.
    The rating scale is from 1 to 5, where:

    1: The clarification and options are very unlikely to engage the user, as they are perceived as irrelevant or useless.
    2: The clarification and options are somewhat likely to engage the user but lack depth or strong relevance.
    3: The clarification and options are moderately likely to engage the user, showing some relevance and usefulness.
    4: The clarification and options are highly likely to engage the user, demonstrating strong relevance and helpfulness.
    5: The clarification and options are exceptionally likely to engage the user, being highly relevant, clear, and useful.


    Query: {query}

    Clarification Questions and their options: {clarifications_str}

    Now, Based on the above query, and Clarification Questions and their options and shown examples, rate the human preference label of each clarification and its options
    on a scale from 1 to 5. You can give the same label to more than one clarification question and its options if you have no preferences. 
    Your response should only contain a single number between 1 and 5 for each clarification and its options, and nothing else.
    Additionally, rate your confidence in your response on a scale of 0% to 100%, where 0% means no confidence and 100% means absolute confidence.
    provide your prediction for the label (a single number between 1 and 5) and your confidence level (as a percentage).
    Make sure you provide Prediction and Confidence score for each Clarification Question with its Options for a given Query.
    Respond strictly in this format:
    Prediction: [1-5]
    Confidence: [0-100]%
     """


    try:
        # Generate predictions using Mistral
        response = generator(
            input_prompt,
            max_length=1600,         # Maximum length of the generated text
            num_return_sequences=1,  # Number of output sequences
        )

        raw_response = response[0]['generated_text']
        print(f"Raw Response Content for Query '{query}':\n{raw_response}")

        # Extract predictions and confidences
        clarification_matches = re.findall(r'Prediction:\s*([1-5])\s*Confidence:\s*(\d+)%', raw_response)
        for prediction, confidence in clarification_matches:
            predictions.append(int(prediction))
            confidences.append(int(confidence))
            querylist.append(query)
            true_labels.append(labels[idx])  # Append the corresponding true label

    except Exception as e:
        print(f"Error for query '{query}': {str(e)}")
        predictions.extend([None])
        confidences.extend([None])
        true_labels.append(None)  # Append None to true_labels for consistency


print("**Done(Prediction)**")

# Filter out invalid predictions and confidences
valid_indices = [i for i, p in enumerate(predictions) if p is not None]
filtered_predictions = [predictions[i] for i in valid_indices]
filtered_confidences = [confidences[i] for i in valid_indices]
filtered_true_labels = [true_labels[i] for i in valid_indices]

print("Predictions:")
print(predictions)
print("Confidences:")
print(confidences)
print(querylist)

# Calculate metrics
if len(filtered_true_labels) > 0:
    # Accuracy
    accuracy = accuracy_score(filtered_true_labels, filtered_predictions)
    print(f"Accuracy: {accuracy:.4f}")

    # Cohen's Kappa
    kappa = cohen_kappa_score(filtered_true_labels, filtered_predictions)
    print(f"Cohenâ€™s Kappa: {kappa:.4f}")

    # Pearson Correlation
    if len(filtered_true_labels) > 1:
        correlation = pearsonr(filtered_true_labels, filtered_predictions)[0]
        print(f"Pearson Correlation: {correlation:.4f}")
    else:
        print("Not enough data points to calculate Pearson correlation.")

    # Average Confidence
    avg_confidence = sum(filtered_confidences) / len(filtered_confidences)
    print(f"Average Confidence: {avg_confidence:.2f}%")

    # Confidence-Weighted Accuracy
    weighted_correctness = sum(
        (filtered_predictions[i] == filtered_true_labels[i]) * (filtered_confidences[i] / 100)
        for i in range(len(filtered_predictions))
    )
    total_weights = sum(filtered_confidences[i] / 100 for i in range(len(filtered_confidences)))
    confidence_weighted_accuracy = weighted_correctness / total_weights if total_weights > 0 else 0
    print(f"Confidence-Weighted Accuracy: {confidence_weighted_accuracy:.4f}")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")

