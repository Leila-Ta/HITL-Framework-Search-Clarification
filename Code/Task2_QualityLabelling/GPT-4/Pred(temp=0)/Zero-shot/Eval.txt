import time
import pandas as pd
import openai
import re
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr

# Load the MIMICS-Duo dataset
data = pd.read_csv('/New Research/DataSet/Task2-QualityLabelling.csv')

queries = data['query']
clarifications = data['question']
labels = data['OverallClarificationPaneQuality']  # This is for comparison after GPT-4 predicts

options = [data['option_1'], data['option_2'], data['option_3'], data['option_4'], data['option_5']]
print("*Done(Data Loading)")

# Set your OpenAI API key
openai.api_key = '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'

predictions = []
confidences = []

# Prepare a sample input for GPT-4
for idx, (query, clarification, *options_row) in enumerate(zip(queries, clarifications, *options)):
    # Construct input prompt for Cohere Command R
    options_str = [str(option) if pd.notna(option) else '' for option in options_row]
    input_prompt = f"""

     A query and associated multi-choice clarification question with its options are shown to you. Based on the query, I want to ask you rate the overall quality of the clarification question and its options on a scale from 1 to 5. 
     Your response should only contain a single number between 1 and 5. Number one means very bad quality, number 2 means bad quality, number 3 means fair quality, number 4 means good quality and number 5 means very good quality. 

     Query: {query}
     Clarification Question: {clarification}
     Options: {', '.join(options_str)}

     Now, Based on the above Query, I want to ask you rate the quality of the Clarification Question and its Options on a scale from 1 to 5. 
     Your response should only contain a single number between 1 and 5 and nothing else.
     Additionally, rate your confidence in your response on a scale of 0% to 100%, where 0% means no confidence and 100% means absolute confidence.
     provide your prediction for the label (a single number between 1 and 5) and your confidence level (as a percentage).
     Respond strictly in this format:
     Prediction: [1-5]
     Confidence: [0-100]%
      """

    try:
        # Use GPT-4 to get the predicted label
        response = openai.ChatCompletion.create(
            model="gpt-4o",  # Assuming "gpt-4" is available in the OpenAI API. You can also use "gpt-3.5-turbo"
            messages=[
                {"role": "user", "content": input_prompt}
            ],
            max_tokens=100,
            temperature=0,
        )

        # Extract response text
        raw_response = response['choices'][0]['message']['content'].strip()
        print(f"Raw Response Content for Prompt {idx + 1}: {raw_response}")

        # Extract response content
        match_prediction = re.search(r'Prediction:\s*([1-5])', raw_response)
        match_confidence = re.search(r'Confidence:\s*(\d+)%', raw_response)

        predicted_label = int(match_prediction.group(1)) if match_prediction else None
        confidence = int(match_confidence.group(1)) if match_confidence else None

        predictions.append(predicted_label)
        confidences.append(confidence)

        print(f"Predicted Label for Prompt {idx + 1}: {predicted_label}")
        print(f"Confidence Level for Prompt {idx + 1}: {confidence}%")
        time.sleep(1)

    except Exception as e:
        # Handle failed API call
        print(f"Error for prompt {idx + 1}: {str(e)}")
        predictions.append(None)
        confidences.append(None)

print("**Done(Prediction)")

# Convert predictions to integers for numerical comparison
predictions = [pred for pred in predictions if pred is not None]
confidences = [conf for conf in confidences if conf is not None]
print("Predictions:")
print(predictions)
print("confidences:")
print(confidences)
print("****Done(Post-Processing Predictions)")

# Convert original labels to integers
true_labels = labels[:len(predictions)].astype(int).tolist()

# Calculate metrics if there are enough predictions
if len(true_labels) > 0:
    accuracy = accuracy_score(true_labels, predictions)
    kappa = cohen_kappa_score(true_labels, predictions)
    correlation = pearsonr(true_labels, predictions)[0] if len(true_labels) > 1 else None

    avg_confidence = sum([c for c in confidences if c is not None]) / len(confidences) if confidences else 0

    print(f'Accuracy: {accuracy:.4f}')
    print(f'Cohen’s Kappa: {kappa:.4f}')
    if correlation is not None:
        print(f'Pearson Correlation: {correlation:.4f}')
    else:
        print("Not enough data points to calculate Pearson correlation.")
    print(f'Average Confidence: {avg_confidence:.2f}%')

    # Calculate confidence-weighted accuracy
    weighted_correctness = 0
    total_weights = 0
    for i, (true_label, pred, conf) in enumerate(zip(true_labels, predictions, confidences)):
        if conf is not None:
            weight = conf / 100
            correctness = 1 if pred == true_label else 0
            weighted_correctness += correctness * weight
            total_weights += weight

    confidence_weighted_accuracy = weighted_correctness / total_weights if total_weights > 0 else 0
    print(f"Confidence-Weighted Accuracy: {confidence_weighted_accuracy:.4f}")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")

----------------------------------------------------------------------

Accuracy: 0.2834
Cohen’s Kappa: 0.0299
Pearson Correlation: 0.2859
Average Confidence: 90.16%
Confidence-Weighted Accuracy: 0.2828
