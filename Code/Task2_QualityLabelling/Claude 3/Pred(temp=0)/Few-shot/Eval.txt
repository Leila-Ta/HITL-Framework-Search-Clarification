import pandas as pd
import re
from anthropic import Anthropic
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr
import time

# Load the MIMICS-Duo dataset
data = pd.read_csv('.../DataSet/Task2-QualityLabelling.csv')

# Take just first 5 samples for testing
queries = data['query']
clarifications = data['question']
labels = data['OverallClarificationPaneQuality']
options = [data['option_1'], data['option_2'], data['option_3'], data['option_4'], data['option_5']]
print("*Done(Data Loading)")

# Set up Anthropic client
API_KEY = '...'
client = Anthropic(api_key=API_KEY)

predictions = []
confidences =[]

# Prepare input for Claude
for idx, (query, clarification, *options_row) in enumerate(zip(queries, clarifications, *options)):
    # Construct input prompt for Claude
    options_str = [str(option) if pd.notna(option) else '' for option in options_row]
    input_prompt = f"""

    A query and associated multi-choice clarification question with its options are shown to you. Based on the query, I want to ask you rate the overall quality of the clarification question and its options on a scale from 1 to 5. 
    Your response should only contain a single number between 1 and 5. Number one means very bad quality, number 2 means bad quality, number 3 means fair quality, number 4 means good quality and number 5 means very good quality. 
    Here are five examples showing how humans (users) labelled the overall quality of clarification questions associated with the queries.
    
    Example 1:
    Query: 2 wheeler insurance
    Clarification question: Select one to refine your search
    Option 1: bharti axa
    Option 2: hdfc
    Option 3: bajaj allianz
    Option 4: icici lombard
    Quality label (Ground Truth Label): 1
    
    Example 2:
    Query: 1 samuel 1
    Clarification question: Select one to refine your search
    Option 1: kjv
    Option 2: nlt
    Option 3: esv
    Option 4: niv
    Option 5: nasb
    Quality label (Ground Truth Label): 2
    
    Example 3:
    Query: 12 gauge shotgun
    Clarification question: Select one to refine your search
    Option 1: bolt action
    Option 2: lever action
    Option 3: single shot
    Quality label (Ground Truth Label): 3
    
    Example 4:
    Query: 123 watch free movies online
    Clarification question: Select one to refine your search
    Option 1: 123 watch free english movies online
    Option 2: action movies
    Option 3: comedy movies
    Option 4: horror movies
    Option 5: drama movies
    Quality label (Ground Truth Label): 4
    
    Example 5:
    Query: 20 questions game
    Clarification question: Select one to refine your search
    Option 1: for couples
    Option 2: for friends
    Option 3: for kids
    Option 4: for teens
    Option 5: for seniors
    Quality label (Ground Truth Label): 5
    
    Query: {query}
    Clarification Question: {clarification}
    Options: {', '.join(options_str)}
    
    Now, Based on the above Query, I want to ask you rate the quality of the Clarification Question and its Options on a scale from 1 to 5. 
    Your response should only contain a single number between 1 and 5 and nothing else.
    Additionally, rate your confidence in your response on a scale of 0% to 100%, where 0% means no confidence and 100% means absolute confidence.
    provide your prediction for the label (a single number between 1 and 5) and your confidence level (as a percentage).
    Respond strictly in this format:
    Prediction: [1-5]
    Confidence: [0-100]%
     """

    try:
        # Make the API request to get a response from Claude
        response = client.messages.create(
            model="claude-3-haiku-20240307",  # Using Haiku model to minimize costs
            messages=[{
                "role": "user",
                "content": input_prompt
            }],
            max_tokens=100,
            temperature=0
        )

        # Extract response text
        raw_response = response.content[0].text.strip()
        print(f"Raw Response Content for Prompt {idx + 1}: {raw_response}")

        # Extract response content
        match_prediction = re.search(r'Prediction:\s*([1-5])', raw_response)
        match_confidence = re.search(r'Confidence:\s*(\d+)%', raw_response)

        predicted_label = int(match_prediction.group(1)) if match_prediction else None
        confidence = int(match_confidence.group(1)) if match_confidence else None

        predictions.append(predicted_label)
        confidences.append(confidence)

        print(f"Predicted Label for Prompt {idx + 1}: {predicted_label}")
        print(f"Confidence Level for Prompt {idx + 1}: {confidence}%")

    except Exception as e:
        # Handle failed API call
        print(f"Error for prompt {idx + 1}: {str(e)}")
        predictions.append(None)
        confidences.append(None)

print("**Done(Prediction)")

# Convert predictions to integers for numerical comparison
predictions = [pred for pred in predictions if pred is not None]
confidences = [conf for conf in confidences if conf is not None]
print("Predictions:")
print(predictions)
print("confidences:")
print(confidences)
print("****Done(Post-Processing Predictions)")

# Convert original labels to integers
true_labels = labels[:len(predictions)].astype(int).tolist()

# Calculate metrics if there are enough predictions
if len(true_labels) > 0:
    accuracy = accuracy_score(true_labels, predictions)
    kappa = cohen_kappa_score(true_labels, predictions)
    correlation = pearsonr(true_labels, predictions)[0] if len(true_labels) > 1 else None

    avg_confidence = sum([c for c in confidences if c is not None]) / len(confidences) if confidences else 0

    print(f'Accuracy: {accuracy:.4f}')
    print(f'Cohenâ€™s Kappa: {kappa:.4f}')
    if correlation is not None:
        print(f'Pearson Correlation: {correlation:.4f}')
    else:
        print("Not enough data points to calculate Pearson correlation.")
    print(f'Average Confidence: {avg_confidence:.2f}%')

    # Calculate confidence-weighted accuracy
    weighted_correctness = 0
    total_weights = 0
    for i, (true_label, pred, conf) in enumerate(zip(true_labels, predictions, confidences)):
        if conf is not None:
            weight = conf / 100
            correctness = 1 if pred == true_label else 0
            weighted_correctness += correctness * weight
            total_weights += weight

    confidence_weighted_accuracy = weighted_correctness / total_weights if total_weights > 0 else 0
    print(f"Confidence-Weighted Accuracy: {confidence_weighted_accuracy:.4f}")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")

