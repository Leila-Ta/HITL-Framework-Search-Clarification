import pandas as pd
import cohere
import re
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr
import time

# Load the MIMICS-Duo dataset
data = pd.read_csv('/New Research/DataSet/Task3-AspectLabelling.csv')

# Take just first 5 samples for testing (you can adjust this for a full run)
# data = data.head(5)

queries = data['query']
clarifications = data['question']
labels = data['Coverage']
# options = [data['option_1']]
options = [data['option_1'], data['option_2'], data['option_3'], data['option_4'], data['option_5']]
print("*Done(Data Loading)")

# Set up Cohere client
API_KEY = 'EWQUf6rOHYMAF00jZm2j0abE28sjIJpPrGNDDe47'
co = cohere.Client(API_KEY)

predictions = []
confidences = []
querylist = []

# Prepare input for Cohere Command R
for idx, (query, clarification, *options_row) in enumerate(zip(queries, clarifications, *options)):
    # Construct input prompt for Cohere Command R
    options_str = [str(option) if pd.notna(option) else '' for option in options_row]

    input_prompt = f"""

        A query and associated multi-choice clarification question with its options are shown to you. A multi-choice clarification question has high coverage if its potential answers cover every potential aspect of the query. You are tasked to rate the extent to which the clarification question and its options cover every potential aspect of the query on a scale from 1 to 5.
    Number 1 means you strongly disagree that the multi-choice clarification question with its options has a high coverage for the given query.
    Number 2 means you somewhat disagree that the multi-choice clarification question with its options has a high coverage for the given query.
    Number 3 means you neither agree nor disagree that the multi-choice clarification question with its options has a high coverage for the given query.
    Number 4 means you somewhat agree that the multi-choice clarification question with its options has a high coverage for the given query.
    Number 5 means you strongly agree that the multi-choice clarification question with its options has a high coverage for the given query.

    Here are five examples showing how humans (users) labelled the coverage of clarification questions associated with the queries.

    Example 1:
    Query: 2019 vw
    Clarification Question: Select one to refine your search
    Option 1: beetle
    Option 2: jetta
    Coverage label (Ground Truth Label): 1

    Example 2:
    Query: 1800 calorie diet
    Clarification Question: Select one to refine your search
    Option 1: printable for men
    Option 2: printable for women
    Coverage label (Ground Truth Label): 2


    Example 3:
    Query: 2k servers
    Clarification Question: Select one to refine your search
    Option 1: nba 2k servers
    Option 2: wwe 2k servers
    Option 3: nfl 2k servers
    Coverage label (Ground Truth Label): 3

    Example 4:
    Query: 2007 honda accord
    Clarification Question: Select one to refine your search
    Option 1: 2007 honda accord interior
    Option 2: 2007 honda accord exterior
    Coverage label (Ground Truth Label): 4

    Example 5:
    Query: a million little things
    Clarification Question: Select one to refine your search
    Option 1: a million little things cast
    Option 2: a million little things trailer
    Option 3: review
    Option 4: episodes
    Option 5: soundtrack
    Coverage label (Ground Truth Label): 5

        Query: {query}
        Clarification Question: {clarification}
        Options: {', '.join(options_str)}

        Now, Based on the above Query, I want to ask you to rate the extent to which the Clarification Question and its Options cover every potential aspect of the query on a scale from 1 to 5. Your response should only contain a single number between 1 and 5 and nothing else.
        Additionally, rate your confidence in your response on a scale of 0% to 100%, where 0% means no confidence and 100% means absolute confidence.
        provide your prediction for the label (a single number between 1 and 5) and your confidence level (as a percentage).
        Respond strictly in this format:
        Prediction: [1-5]
        Confidence: [0-100]%

         """

    try:
        # Make the API request to get a response from Cohere
        response = co.generate(
            model='command-xlarge',
            prompt=input_prompt,
            max_tokens=250,
            temperature=0,
        )

        # Extract response text
        raw_response = response.generations[0].text.strip()
        print(f"Raw Response Content for Prompt {idx + 1}: {raw_response}")

        # Extract response content
        match_prediction = re.search(r'Prediction:\s*([1-5])', raw_response)
        match_confidence = re.search(r'Confidence:\s*(\d+)%', raw_response)

        predicted_label = int(match_prediction.group(1)) if match_prediction else None
        confidence = int(match_confidence.group(1)) if match_confidence else None

        predictions.append(predicted_label)
        confidences.append(confidence)
        querylist.append(query)

        print(f"Predicted Label for Prompt {idx + 1}: {predicted_label}")
        print(f"Confidence Level for Prompt {idx + 1}: {confidence}%")

    except Exception as e:
        # Handle failed API call
        print(f"Error for prompt {idx + 1}: {str(e)}")
        predictions.append(None)
        confidences.append(None)

print("**Done(Prediction)")

# Convert predictions to integers for numerical comparison
predictions = [pred for pred in predictions if pred is not None]
confidences = [conf for conf in confidences if conf is not None]
print("Predictions:")
print(predictions)
print("confidences:")
print(confidences)
print("****Done(Post-Processing Predictions)")
print(querylist)

# Convert original labels to integers
true_labels = labels[:len(predictions)].astype(int).tolist()

# Calculate metrics if there are enough predictions
if len(true_labels) > 0:
    accuracy = accuracy_score(true_labels, predictions)
    kappa = cohen_kappa_score(true_labels, predictions)
    correlation = pearsonr(true_labels, predictions)[0] if len(true_labels) > 1 else None

    avg_confidence = sum([c for c in confidences if c is not None]) / len(confidences) if confidences else 0

    print(f'Accuracy: {accuracy:.4f}')
    print(f'Cohenâ€™s Kappa: {kappa:.4f}')
    if correlation is not None:
        print(f'Pearson Correlation: {correlation:.4f}')
    else:
        print("Not enough data points to calculate Pearson correlation.")
    print(f'Average Confidence: {avg_confidence:.2f}%')

    # Calculate confidence-weighted accuracy
    weighted_correctness = 0
    total_weights = 0
    for i, (true_label, pred, conf) in enumerate(zip(true_labels, predictions, confidences)):
        if conf is not None:
            weight = conf / 100
            correctness = 1 if pred == true_label else 0
            weighted_correctness += correctness * weight
            total_weights += weight

    confidence_weighted_accuracy = weighted_correctness / total_weights if total_weights > 0 else 0
    print(f"Confidence-Weighted Accuracy: {confidence_weighted_accuracy:.4f}")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")

