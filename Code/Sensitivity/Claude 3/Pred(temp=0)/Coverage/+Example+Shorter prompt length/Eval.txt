import pandas as pd
import re
from anthropic import Anthropic
from sklearn.metrics import accuracy_score, cohen_kappa_score
from scipy.stats import pearsonr
import time

# Load the MIMICS-Duo dataset
data = pd.read_csv('/New Research/DataSet/Task3-AspectLabelling.csv')

# Take just first 5 samples for testing
queries = data['query']
clarifications = data['question']
labels = data['Coverage']
options = [data['option_1'], data['option_2'], data['option_3'], data['option_4'], data['option_5']]
print("*Done(Data Loading)")

# Set up Anthropic client
API_KEY = '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'
client = Anthropic(api_key=API_KEY)

predictions = []
confidences = []
querylist = []

# Prepare input for Claude
for idx, (query, clarification, *options_row) in enumerate(zip(queries, clarifications, *options)):
    # Construct input prompt for Claude
    options_str = [str(option) if pd.notna(option) else '' for option in options_row]
    input_prompt = f"""

A query and its associated multi-choice clarification question with options are shown below. A clarification question has high coverage if its options address every potential aspect of the query. Rate the coverage on a scale from 1 to 5:

1: Strongly disagree (low coverage)
2: Somewhat disagree
3: Neutral
4: Somewhat agree
5: Strongly agree (high coverage)
Examples:

Query: 2019 vw
Clarification: Select one to refine your search
Options: beetle, jetta
Coverage Label: 1

Query: 1800 calorie diet
Clarification: Select one to refine your search
Options: printable for men, printable for women
Coverage Label: 2

Query: 2k servers
Clarification: Select one to refine your search
Options: nba 2k servers, wwe 2k servers, nfl 2k servers
Coverage Label: 3

Query: 2007 honda accord
Clarification: Select one to refine your search
Options: interior, exterior
Coverage Label: 4

Query: a million little things
Clarification: Select one to refine your search
Options: cast, trailer, review, episodes, soundtrack
Coverage Label: 5

Evaluate the Following:
Query: {query}
Clarification: {clarification}
Options: {', '.join(options_str)}

Provide:

Prediction: A single number (1-5)
Confidence: Percentage (0%-100%)
Respond in this format:
Prediction: [1-5]
Confidence: [0-100]%

         """

    try:
        # Make the API request to get a response from Claude
        response = client.messages.create(
            model="claude-3-haiku-20240307",  # Using Haiku model to minimize costs
            messages=[{
                "role": "user",
                "content": input_prompt
            }],
            max_tokens=1000,
            temperature=0
        )

        # Extract response text
        raw_response = response.content[0].text.strip()
        print(f"Raw Response Content for Prompt {idx + 1}: {raw_response}")

        # Extract response content
        match_prediction = re.search(r'Prediction:\s*([1-5])', raw_response)
        match_confidence = re.search(r'Confidence:\s*(\d+)%', raw_response)

        predicted_label = int(match_prediction.group(1)) if match_prediction else None
        confidence = int(match_confidence.group(1)) if match_confidence else None

        predictions.append(predicted_label)
        confidences.append(confidence)
        querylist.append(query)

        print(f"Predicted Label for Prompt {idx + 1}: {predicted_label}")
        print(f"Confidence Level for Prompt {idx + 1}: {confidence}%")

    except Exception as e:
        # Handle failed API call
        print(f"Error for prompt {idx + 1}: {str(e)}")
        predictions.append(None)
        confidences.append(None)

print("**Done(Prediction)")

# Convert predictions to integers for numerical comparison
predictions = [pred for pred in predictions if pred is not None]
confidences = [conf for conf in confidences if conf is not None]
print("Predictions:")
print(predictions)
print("confidences:")
print(confidences)
print("****Done(Post-Processing Predictions)")
print(querylist)

# Convert original labels to integers
true_labels = labels[:len(predictions)].astype(int).tolist()

# Calculate metrics if there are enough predictions
if len(true_labels) > 0:
    accuracy = accuracy_score(true_labels, predictions)
    kappa = cohen_kappa_score(true_labels, predictions)
    correlation = pearsonr(true_labels, predictions)[0] if len(true_labels) > 1 else None

    avg_confidence = sum([c for c in confidences if c is not None]) / len(confidences) if confidences else 0

    print(f'Accuracy: {accuracy:.4f}')
    print(f'Cohenâ€™s Kappa: {kappa:.4f}')
    if correlation is not None:
        print(f'Pearson Correlation: {correlation:.4f}')
    else:
        print("Not enough data points to calculate Pearson correlation.")
    print(f'Average Confidence: {avg_confidence:.2f}%')

    # Calculate confidence-weighted accuracy
    weighted_correctness = 0
    total_weights = 0
    for i, (true_label, pred, conf) in enumerate(zip(true_labels, predictions, confidences)):
        if conf is not None:
            weight = conf / 100
            correctness = 1 if pred == true_label else 0
            weighted_correctness += correctness * weight
            total_weights += weight

    confidence_weighted_accuracy = weighted_correctness / total_weights if total_weights > 0 else 0
    print(f"Confidence-Weighted Accuracy: {confidence_weighted_accuracy:.4f}")
else:
    print("Not enough valid predictions to calculate metrics.")

print("*****Done(Comparing)*****")

